{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Vessel segmentation in retina fundus images (revisited)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "In this assignment, we are going to develop a system to automatically **segment vessels** in human retina fundus images (again).\n",
    "\n",
    "In assignment 2 we trained a classical classification algorithm based on a Gaussian filterbank and a kNN classifier.\n",
    "This time we are going to use convolutional neural networks to solve the same task!\n",
    "\n",
    "We are going to use the same data from the publicly available DRIVE dataset (http://www.isi.uu.nl/Research/Databases/DRIVE/).\n",
    "The DRIVE dataset consists of 40 images, 20 used for training and 20 used for testing. Each case contains:\n",
    "* fundus (RGB) image\n",
    "* a binary mask, which indicates the area of the image that has to be analyzed (removing black background)\n",
    "* manual annotations of retina vessels, provided as a binary map.\n",
    "\n",
    "You can download the data for this assignment from this link: https://surfdrive.surf.nl/files/index.php/s/VZnaAZ8GWTZCCka (password: ismi2017)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Tasks for this assignment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "1. Develop a system to segment vessels in retina image in this notebook. You will have to submit this notebook with your code, which we will run and evaluate, together with the results of the segmentation.\n",
    "2. Use the training set provided with the DRIVE dataset to train/tune the parameters of your system. You cannot use data from the test set available on the DRIVE website, nor from other datasets. \n",
    "3. Apply it to the test dataset and generate a binary map of the segmented vessel. The map must have the same size as the input image.\n",
    "4. Submit the results of the notebook to the mini-challenge framework by running the corresponding cell in this notebook (at the end of the notebook). This will automatically submit your results to the mini-challenge framework. You will be able to visualize your scores in the **CHALLENGE** section of the webpage (http://ismi17.diagnijmegen.nl/). Note that after you submit the notebook, we will run your implementation and reproduce your result in order to evaluate your assignment. Any significant discrepancy between the results submitted to the mini-challenge framework and the one computed using this notebook will be penalized and discussed with the student."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Mandatory tasks\n",
    "* Implement the ```Sample_extractor``` class\n",
    "    * Random rotation augmentation\n",
    "    * Random flipping augmentation\n",
    "    \n",
    "    \n",
    "* Implement the ```Batch_extractor``` class\n",
    "\n",
    "\n",
    "* Implement a fully convolutional network\n",
    "    * Train your network\n",
    "\n",
    "\n",
    "* Implement the shift and stitch method\n",
    "\n",
    "* Submit your results to ```challengr```\n",
    "\n",
    "### Optional tasks\n",
    "* Implement additional data augmentation strategies and show if they can improve the performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "\n",
    "## Implementation \n",
    "\n",
    "First we import some neccesary libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# import libraries needed for this assignment\n",
    "import os\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from tqdm import tnrange, tqdm_notebook\n",
    "from scipy.ndimage.interpolation import affine_transform\n",
    "from math import cos, sin, radians, floor, ceil\n",
    "\n",
    "import theano\n",
    "import theano.tensor as T\n",
    "import lasagne\n",
    "import lasagne.layers as L\n",
    "\n",
    "import random\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from matplotlib import cm\n",
    "%matplotlib inline\n",
    "matplotlib.rcParams['figure.figsize'] = (20, 12)\n",
    "from IPython import display\n",
    "import time\n",
    "from tqdm import tnrange, tqdm_notebook\n",
    "\n",
    "from challenger import submit_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# function to get a list of file of a given extension, both the absolute path and the filename\n",
    "def get_file_list(path,ext='',queue=''):\n",
    "    if ext != '':\n",
    "        return [os.path.join(path,f) for f in os.listdir(path) if f.endswith(''+queue+'.'+ext+'')],  [f for f in os.listdir(path) if f.endswith(''+queue+'.'+ext+'')]    \n",
    "    else:\n",
    "        return [os.path.join(path,f) for f in os.listdir(path)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Point to the local copy of training data.\n",
    "\n",
    "Set ```data_dir``` to the location of the folder containing the DRIVE data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "data_dir = \"\"\n",
    "\n",
    "tra_img_dir = os.path.join(data_dir, 'training', 'images')\n",
    "tra_msk_dir = os.path.join(data_dir, 'training', 'mask')\n",
    "tra_lbl_dir = os.path.join(data_dir, 'training', '1st_manual')\n",
    "\n",
    "tra_imgs_all = sorted(get_file_list(tra_img_dir, 'tif')[0])\n",
    "tra_msks_all = sorted(get_file_list(tra_msk_dir, 'gif')[0])\n",
    "tra_lbls_all = sorted(get_file_list(tra_lbl_dir, 'gif')[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Split the data in a training and validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# split training set in a training and validation set\n",
    "# >>> YOUR CODE STARTS HERE <<<\n",
    "validation_percentage = None\n",
    "# >>> YOUR CODE ENDS HERE <<<  \n",
    "\n",
    "n_validation_images = int(ceil(len(tra_imgs_all)*validation_percentage))\n",
    "\n",
    "# use the first images as validation\n",
    "val_imgs = tra_imgs_all[0:n_validation_images]\n",
    "val_msks = tra_msks_all[0:n_validation_images]\n",
    "val_lbls = tra_lbls_all[0:n_validation_images]\n",
    "\n",
    "# the rest as training\n",
    "tra_imgs = tra_imgs_all[n_validation_images:]\n",
    "tra_msks = tra_msks_all[n_validation_images:]\n",
    "tra_lbls = tra_lbls_all[n_validation_images:]\n",
    "\n",
    "print \"nr of training images: \" + str(len(tra_imgs)) + \"\\nnr of validation images: \" + str(len(val_imgs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Define a function to visualize (1) the fundus image, (2) the binary mask, (3) the manual annotation of a case with a given index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def show_image(idx, imgs, msks, lbls):\n",
    "    img = np.asarray(Image.open(imgs[idx]))\n",
    "    print img.shape\n",
    "    msk = np.asarray(Image.open(msks[idx]))\n",
    "    lbl = np.asarray(Image.open(lbls[idx]))\n",
    "    plt.subplot(1,3,1)\n",
    "    plt.imshow(img); plt.title('RGB image {}'.format(idx+1))\n",
    "    plt.subplot(1,3,2)\n",
    "    plt.imshow(msk, cmap='gray'); plt.title('Mask {}'.format(idx+1))\n",
    "    plt.subplot(1,3,3)\n",
    "    plt.imshow(lbl, cmap='gray'); plt.title('Manual annotation {}'.format(idx+1))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Show the training set using the function defined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for i in range(len(tra_imgs)):\n",
    "    show_image(i, tra_imgs, tra_msks, tra_lbls)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Build a sample extractor\n",
    "Differently from what done in the past two weeks, in this assignment we are going to implement a 'SampleExtractor' class, which extract random samples from a list of images, in order to **generate mini-batches on-the-fly**.\n",
    "This means that you don't need to create a dataset beforehand and then use it to train you network, but you will just have a list of training images available, and patches will be extracted on-the-fly during training.\n",
    "This strategy allows to save time in the preparation of your *static* dataset, and allows the use of a *dynamic* generation of mini-batches, where data augmentation can also be applied on the fly.\n",
    "Note that this approach allows to test different strategies of data augmentation without the need for making a new dataset from scratch all the time.\n",
    "We will implement two kind of data augmentation in the class ```SampleExtractor```:\n",
    "* patch rotation\n",
    "* patch flipping\n",
    "\n",
    "In both cases, the event will occur at random, meaning that during patch extraction, some of the extracted patches will be randomly transformed. Furthermore, the rotation angle will also be defined randomly, which means that the combination of different patches used for training is in practice infinite (of course there is a limitation at some point)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "class SampleExtractor:\n",
    "\n",
    "    def __init__(self, patch_size, imgs, msks, lbls, max_rotation=0, rnd_flipping=False):\n",
    "        self.patch_size = patch_size  #y,x\n",
    "        self.img_array = np.zeros([len(imgs), 584, 565])\n",
    "        self.msk_array = np.zeros([len(msks), 584, 565])\n",
    "        self.lbl_array = np.zeros([len(lbls), 584, 565])\n",
    "        \n",
    "        # parameters used for data augmentation on-the-fly\n",
    "        self.max_rotation = max_rotation # float\n",
    "        self.rnd_flipping = rnd_flipping # boolean\n",
    "     \n",
    "        ## load images, masks, and labels in memory\n",
    "        for i, (img, msk, lbl) in enumerate(zip(imgs, msks, lbls)):\n",
    "            self.img_array[i] = np.asarray(Image.open(img))[:,:,1] # green channel\n",
    "            self.msk_array[i] = np.asarray(Image.open(msk))/255.0 # (0, 1)\n",
    "            self.lbl_array[i] = np.asarray(Image.open(lbl))\n",
    "            \n",
    "        ## precalculate positive and negative indices inside the retina mask\n",
    "        self.pos_idx = np.array(np.where(np.logical_and(self.lbl_array > 0, self.msk_array == 1))) #indexed as [image, y, x]\n",
    "        self.neg_idx = np.array(np.where(np.logical_and(self.lbl_array == 0, self.msk_array == 1)))  #indexed as [image, y, x]\n",
    "        \n",
    "        print 'shape of positive indexes: {}'.format(self.pos_idx.shape)\n",
    "        print 'shape of negative indexes: {}'.format(self.neg_idx.shape)\n",
    "                \n",
    "    def get_rnd_rotation(self):\n",
    "        ''' Return a 2x2 rotation matrix to rotate by a random angle phi. \n",
    "            The angle should in the range of -self.max_rotations to +self.max_rotations. '''\n",
    "        if self.max_rotation == 0:\n",
    "            return np.eye(2) # identity matrix\n",
    "        else:\n",
    "            #create a random rotation matrix 'rot_mat' in the range of -'self.max_rotations' to +'self.max_rotations'\n",
    "                        \n",
    "            # >>> YOUR CODE STARTS HERE <<<\n",
    "            # >>> YOUR CODE ENDS HERE <<<   \n",
    "            \n",
    "            return rot_mat # 2x2 np.array    \n",
    "    \n",
    "    def rnd_flip(self, X):\n",
    "        ''' Return a flipped version of the input patch X.\n",
    "            The kind of flip and the probability that the event happens\n",
    "            should be based on a random event. '''       \n",
    "        \n",
    "        # write a function that randomly flips the input image\n",
    "        # >>> YOUR CODE STARTS HERE <<<\n",
    "        # >>> YOUR CODE ENDS HERE <<<   \n",
    "        return X\n",
    "    \n",
    "    def extract_sample(self, image, loc_y, loc_x):\n",
    "        ps_y, ps_x = self.patch_size\n",
    "        \n",
    "        ## get a random rotation matrix\n",
    "        rot_mat = self.get_rnd_rotation()\n",
    "        \n",
    "        ## Shift coordinate system to the center of the image\n",
    "        ## for more information http://stackoverflow.com/questions/20161175/how-can-i-use-scipy-ndimage-interpolation-affine-transform-to-rotate-an-image-ab\n",
    "        c_in = loc_y, loc_x\n",
    "        c_out = np.array([ps_y/2, ps_x/2])\n",
    "        offset = c_in - c_out.dot(rot_mat)\n",
    "        \n",
    "        ## extract the patch and apply the rotation matrix\n",
    "        X = affine_transform(self.img_array[image], rot_mat.T, offset=offset, output_shape=(ps_y, ps_x), order=1)/255.0   \n",
    "        ## extract the label\n",
    "        Y = int(self.lbl_array[image,loc_y,loc_x] > 0)\n",
    "        \n",
    "        ## apply random flipping\n",
    "        if self.rnd_flipping:\n",
    "            X = self.rnd_flip(X)\n",
    "            \n",
    "        return X, Y       \n",
    "           \n",
    "    def get_random_sample_from_class(self, label):\n",
    "        ''' Extract a patch with a given label. '''\n",
    "        \n",
    "        # >>> YOUR CODE STARTS HERE <<<\n",
    "        # tip: use 'self.pos_idx' and 'self.neg_idx'\n",
    "        if label == 0: \n",
    "            image, loc_y, loc_x = None, None, None\n",
    "        else:\n",
    "            image, loc_y, loc_x = None, None, None\n",
    "        # >>> YOUR CODE ENDS HERE <<<   \n",
    "        \n",
    "        return self.extract_sample(image, loc_y, loc_x) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "A convenience function to visualize the output of the patch extractor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def visualize_samples(sampling_function, label):\n",
    "    for i in range(5):\n",
    "        X, Y = sampling_function(label)\n",
    "        plt.subplot(1,5,i+1)\n",
    "        plt.imshow(X, cmap='bone', vmin=0, vmax=1); plt.title('patch: ' + str(i) + \" , class: \" + str(Y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Create the basic random sample extractor\n",
    "Implement the ```get_random_sample_from_class()``` function in the class above. You can use the functions below to test if the output makes sense.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Here we test the patch extractor without random rotation and without flipping.\n",
    "\n",
    "# define a random sample extractor object\n",
    "random_sample_extractor = SampleExtractor((31,31),tra_imgs, tra_msks, tra_lbls, max_rotation=0, rnd_flipping=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Execute the following cells several times to see if the results you get make sense."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# visualize examples of background (label = 0)\n",
    "visualize_samples(random_sample_extractor.get_random_sample_from_class, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# visualize example of vessels (label = 1)\n",
    "visualize_samples(random_sample_extractor.get_random_sample_from_class, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Implement rotation augmentation\n",
    "\n",
    "Implement the ```get_rnd_rotation()``` function in the class above.\n",
    "You can use the functions below to test if the output makes sense."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#Now we enable rotation, and test it again.\n",
    "random_sample_extractor = SampleExtractor((31,31),tra_imgs, tra_msks, tra_lbls, max_rotation=15, rnd_flipping=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# visualize example of vessels (label = 1)\n",
    "visualize_samples(random_sample_extractor.get_random_sample_from_class, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# visualize example of background (label = 0)\n",
    "visualize_samples(random_sample_extractor.get_random_sample_from_class, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Implement mirroring augmentation\n",
    "Implement the ```rnd_flip()`` function in the class above.\n",
    "You can use the functions below to test if the output makes sense."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "random_sample_extractor = SampleExtractor((31,31),tra_imgs, tra_msks, tra_lbls, max_rotation=0, rnd_flipping=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# visualize example of vessels (label = 1)\n",
    "visualize_samples(random_sample_extractor.get_random_sample_from_class, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# visualize example of background (label = 0)\n",
    "visualize_samples(random_sample_extractor.get_random_sample_from_class, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Implement a batch extractor \n",
    "\n",
    "We now implement a batch extractor. This class is based on the sample extractor, but it just makes mini-batches of random samples. The mini-batches will be used to train your network, and as we have seen, there will be random data augmentation there."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "class BatchExtractor:\n",
    "\n",
    "    def __init__(self, sample_extractor, batch_size, class_balancing = False):\n",
    "        self.batch_size = batch_size\n",
    "        self.class_balancing = class_balancing\n",
    "        self.sample_extractor = sample_extractor\n",
    "        \n",
    "    def convert_to_onehot(self, Y, num_classes):  #convert a [b,1,h,w] label tensor to a [b,number_of_categories,h,w] tensor in one_hot form\n",
    "        one_hot = np.zeros((Y.shape[0], num_classes, Y.shape[2], Y.shape[3]))\n",
    "        for i in range(num_classes):\n",
    "            one_hot[:,i:i+1,:,:] = Y.astype(np.int) == i              \n",
    "        return one_hot\n",
    "        \n",
    "    def get_random_batch_balanced(self):\n",
    "        se = self.sample_extractor\n",
    "        \n",
    "        # >>> YOUR CODE STARTS HERE <<<\n",
    "        # Write a function to extract a batch with balanced classes using the previously created patch extractor \n",
    "        # Use the 'get_random_patch_from_class' function you implemented\n",
    "        # >>> YOUR CODE ENDS HERE <<<   \n",
    "        \n",
    "        # X_batch should be a 4D array indexed as [batchsize, channels, y, x]\n",
    "        # Y_batch should be a 4D adday indexed as [batchsize, 1, 1, 1] and will be [batchsize, 2, 1, 1] after onehot conversion\n",
    "        # the batch should contains 50% positive patches and 50% negative patches\n",
    "        \n",
    "        return X_batch, self.convert_to_onehot(Y_batch,2)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Build the fully convolutional CNN\n",
    "\n",
    "We start with defining the network architecture in Lasagne.\n",
    "The network we are using is a socalled **fully convolutional network**, a network that does not contain any fully connected layers. This type of network is well suited for image segmentation as a very efficient implementation can be used. \n",
    "\n",
    "A direct, but naive approach to pixel classification using a CNN would be to extract a patch around every pixel and assign the output back to the pixel location. This naive approach is computationally inefficient, because when we move our 31x31 patch by\n",
    "one pixel to classify the next pixel, 30x31 pixels of the new patch are identical to the previous patch, and we would apply the same convolutions many times over.\n",
    "\n",
    "\n",
    "As the network we use here only contains convolutional filters, i.e. the spatial structure is kept intact, the whole network can be thought of as a single large convolutional filter that can be applied to the whole image at once. This speeds up the classification greatly and allows full image segmentation in seconds!\n",
    "\n",
    "What is important for fully convolutional networks is that the size of the feature maps within the network goes down to exactly 1 for the final feature map. This requires some computation, but luckily it is pretty straightforward. \n",
    "\n",
    "Assume:\n",
    "\n",
    "    - i = Input featuremap size\n",
    "    - f = Convolution filter size (uneven)\n",
    "    - max pooling size = 2\n",
    "    \n",
    "\n",
    "Then:\n",
    "Output size of a feature map  after convolution with a convolution filter of size f:\n",
    "\n",
    "    output_size = input_size -(f-1)\n",
    "\n",
    "Output size of a feature map  after max pooling by a factor 2:\n",
    "\n",
    "    output_size = floor(input_size/2)\n",
    "    \n",
    "\n",
    "Let's define a baseline model:\n",
    "* input layer (given)\n",
    "* 32 filters of 3x3\n",
    "* 32 filters of 3x3\n",
    "* pooling\n",
    "* 64 filters of 3x3\n",
    "* 64 filters of 3x3\n",
    "* pooling\n",
    "* 128 filters of ?x?\n",
    "* 64 filters of 1x1\n",
    "* 2 filters of 1x1 (given)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Define your network builder function. \n",
    "\n",
    "def build_network(input_tensor):\n",
    "    # define the inputlayer\n",
    "    network = L.InputLayer(shape=(None, 1, None, None), input_var = input_tensor)\n",
    "    \n",
    " \n",
    "\n",
    "    # >>> YOUR CODE STARTS HERE <<<\n",
    "    # Implement the fully convolutional network\n",
    "    # >>> YOUR CODE ENDS HERE <<< \n",
    "\n",
    "\n",
    "\n",
    "    # define the final layer. \n",
    "    # because we are now creating a fully convolutional layer, the last layer does NOT contain a softmax function.\n",
    "    # the softmax function will be implemented later in a different way allowing a pixelwise loss\n",
    "    final_layer = L.Conv2DLayer(network, 2, (1,1), \n",
    "                                        nonlinearity=lasagne.nonlinearities.identity, \n",
    "                                        W=lasagne.init.GlorotUniform())    \n",
    "    return final_layer\n",
    "\n",
    "def softmax(network):\n",
    "    output = lasagne.layers.get_output(network)\n",
    "    exp = T.exp(output - output.max(axis=1, keepdims=True)) #subtract max for numeric stability (overflow)\n",
    "    return exp / exp.sum(axis=1, keepdims=True)\n",
    "\n",
    "def softmax_deterministic(network):\n",
    "    output = lasagne.layers.get_output(network, deterministic=True)\n",
    "    exp = T.exp(output - output.max(axis=1, keepdims=True)) #subtract max for numeric stability (overflow)\n",
    "    return exp / exp.sum(axis=1, keepdims=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Define the training, validation and evaluation function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def training_function(network, input_tensor, target_tensor, learning_rate, use_l2_regularization=False, l2_lambda=0.000001):\n",
    "    \n",
    "    # Get the network output and calculate metrics.\n",
    "    network_output = softmax(network)\n",
    "        \n",
    "    if use_l2_regularization:\n",
    "        l2_loss = lasagne.regularization.regularize_network_params(network, lasagne.regularization.l2)\n",
    "        loss = lasagne.objectives.categorical_crossentropy(network_output, target_tensor).mean() + l2_lambda * l2_loss\n",
    "    else:\n",
    "        loss = lasagne.objectives.categorical_crossentropy(network_output, target_tensor).mean()\n",
    "        \n",
    "    accuracy = T.mean(T.eq(T.argmax(network_output, axis=1), T.argmax(target_tensor,axis=1)), dtype=theano.config.floatX)\n",
    "    \n",
    "    # Get the network parameters and the update function.                      \n",
    "    network_params = L.get_all_params(network, trainable=True)\n",
    "    weight_updates = lasagne.updates.sgd(loss, network_params, learning_rate=learning_rate)\n",
    "    \n",
    "    # Construct the training function.\n",
    "    return theano.function([input_tensor, target_tensor], [loss, accuracy], updates=weight_updates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def validate_function(network, input_tensor, target_tensor):\n",
    "    \n",
    "    # Get the network output and calculate metrics.\n",
    "    network_output = softmax_deterministic(network)\n",
    "    loss = lasagne.objectives.categorical_crossentropy(network_output, target_tensor).mean()\n",
    "    accuracy = T.mean(T.eq(T.argmax(network_output, axis=1), T.argmax(target_tensor,axis=1)), dtype=theano.config.floatX)  \n",
    "    \n",
    "    # Construct the validation function.\n",
    "    return theano.function([input_tensor, target_tensor], [loss, accuracy])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def evaluate_function(network, input_tensor):\n",
    " \n",
    "    # Get the network output and calculate metrics.\n",
    "    network_output = softmax_deterministic(network)\n",
    "    \n",
    "    # Construct the evaluation function.\n",
    "    return theano.function([input_tensor], network_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Set parameters\n",
    "Set the parameters to suitable values.\n",
    "Certain paramters have a large influence on the performance of your network, try to find the optimal parameters for this problem!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "learning_rate = None\n",
    "nr_epochs = None\n",
    "nr_iterations_per_epoch = None\n",
    "patch_size = (None,None)\n",
    "batch_size = None\n",
    "max_rotation = None\n",
    "flipping = None\n",
    "class_balancing = None\n",
    "nr_validation_samples = None\n",
    "validation_batch_size = None\n",
    "network_name = 'None'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Compile the theano functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "inputs = T.ftensor4('X') \n",
    "targets = T.ftensor4('Y')\n",
    "\n",
    "network = build_network(inputs)\n",
    "\n",
    "train_fn = training_function(network=network, input_tensor=inputs, target_tensor=targets, learning_rate=learning_rate,\n",
    "                             use_l2_regularization=False, l2_lambda=0.000001)\n",
    "validation_fn = validate_function(network=network, input_tensor=inputs, target_tensor=targets)\n",
    "evaluation_fn = evaluate_function(network=network, input_tensor=inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Create the \n",
    " - sample extractors\n",
    " - batch extractors\n",
    " - constant validation set\n",
    " - Initialize all monitoring statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "random.seed(0)\n",
    "\n",
    "sample_ex_train = SampleExtractor(patch_size, tra_imgs, tra_msks, tra_lbls, max_rotation=max_rotation, rnd_flipping=flipping)\n",
    "batch_ex_train = BatchExtractor(sample_ex_train, batch_size, class_balancing=class_balancing)\n",
    "\n",
    "sample_ex_val = SampleExtractor(patch_size, val_imgs, val_msks, val_lbls, max_rotation=max_rotation, rnd_flipping=flipping)\n",
    "batch_ex_val = BatchExtractor(sample_ex_val, nr_validation_samples, class_balancing=class_balancing)\n",
    "\n",
    "validation_X, validation_Y = batch_ex_val.get_random_batch_balanced()\n",
    "\n",
    "tra_loss_lst = []\n",
    "val_loss_lst= []\n",
    "tra_acc_lst = []\n",
    "val_acc_lst = []\n",
    "best_val_acc = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Main training loop\n",
    "\n",
    "Now that we have a pipeline for creating batches on the fly, we can now train our fully convolutional network!\n",
    "The following code trains for a certain amount of time, defined in the parameters section above, and will then evaluate the performance on our validation set. After plotting the results, the next epoch will start.\n",
    "Try to optimize the parameters to get the best performance possible! (>90% accuracy should be possible!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Main training loop\n",
    "for epoch in range(nr_epochs):\n",
    "    print('Epoch {}'.format(epoch+1))\n",
    "    # training\n",
    "    tra_losses = []\n",
    "    tra_accs = []\n",
    "    print('training...')\n",
    "    for b in tqdm_notebook(range(0, nr_iterations_per_epoch), leave=False):\n",
    "        X, Y = batch_ex_train.get_random_batch_balanced()\n",
    "        loss, accuracy = train_fn(X.astype(np.float32), Y.astype(np.float32))\n",
    "        tra_losses.append(loss)\n",
    "        tra_accs.append(accuracy)\n",
    "    tra_loss_lst.append(np.mean(tra_losses))\n",
    "    tra_acc_lst.append(np.mean(tra_accs))\n",
    "            \n",
    "    # validation\n",
    "    val_losses = []\n",
    "    val_accs = []\n",
    "    print('validation...')\n",
    "    for b in tqdm_notebook(range(0, nr_validation_samples, validation_batch_size), leave=False):\n",
    "        X = validation_X[b:min(b+validation_batch_size, nr_validation_samples)]\n",
    "        Y = validation_Y[b:min(b+validation_batch_size, nr_validation_samples)]\n",
    "        loss, accuracy = validation_fn(X.astype(np.float32), Y.astype(np.float32))\n",
    "        print loss\n",
    "        val_losses.append(loss)\n",
    "        val_accs.append(accuracy)\n",
    "    val_loss_lst.append(np.mean(val_losses))\n",
    "    val_acc_lst.append(np.mean(val_accs))   \n",
    "    #continue\n",
    "    if np.mean(val_accs) > best_val_acc:\n",
    "        best_val_acc = np.mean(val_accs)\n",
    "        # save network\n",
    "        params = L.get_all_param_values(network)\n",
    "        np.savez(os.path.join('./', network_name+'.npz'), params=params)\n",
    "\n",
    "    # plot learning curves\n",
    "    fig = plt.figure(figsize=(10, 5))\n",
    "    tra_loss_plt, = plt.plot(range(len(tra_loss_lst)), tra_loss_lst, 'b')\n",
    "    val_loss_plt, = plt.plot(range(len(val_loss_lst)), val_loss_lst, 'g')\n",
    "    tra_acc_plt, = plt.plot(range(len(tra_acc_lst)), tra_acc_lst, 'm')\n",
    "    val_acc_plt, = plt.plot(range(len(val_acc_lst)), val_acc_lst, 'r')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.ylabel('loss')\n",
    "    plt.legend([tra_loss_plt, val_loss_plt, tra_acc_plt, val_acc_plt], \n",
    "                ['training loss', 'validation loss', 'training accuracy', 'validation accuracy'],\n",
    "                loc='center left', bbox_to_anchor=(1, 0.5))\n",
    "    plt.title('Best validation accuracy = {:.2f}%'.format(100. * best_val_acc))\n",
    "    display.clear_output(wait=True)\n",
    "    display.display(plt.gcf())\n",
    "    time.sleep(.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Evaluate the performance on the test set and submit to Challenger\n",
    "\n",
    "We can now apply the trained fully convolutional network on our independent test set.\n",
    "\n",
    "Load the images:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# test images\n",
    "tes_img_dir = os.path.join(data_dir, 'test', 'images')\n",
    "tes_msk_dir = os.path.join(data_dir, 'test', 'mask')\n",
    "\n",
    "tes_imgs = sorted(get_file_list(tes_img_dir, 'tif')[0])\n",
    "tes_msks = sorted(get_file_list(tes_msk_dir, 'gif')[0])\n",
    "\n",
    "result_output_folder = os.path.join(data_dir, 'test', 'results')\n",
    "if not(os.path.exists(result_output_folder)):\n",
    "    os.mkdirs(result_output_folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Apply the trained network to the images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# indicate the name of the network for this test\n",
    "network_name = 'test'\n",
    "\n",
    "# initialize the network used in this experiment (this may change)\n",
    "network = build_network(inputs)\n",
    "npz = np.load('./'+network_name+'.npz') # load stored parameters\n",
    "lasagne.layers.set_all_param_values(network, npz['params']) # set parameters\n",
    "\n",
    "# one forward pass on the test set\n",
    "threshold = 0.5\n",
    "for f in range(len(tes_imgs)):    \n",
    "    \n",
    "    # open image\n",
    "    img = np.asarray(Image.open(tes_imgs[f]))\n",
    "    print img.shape\n",
    "    \n",
    "    # exrtact green channel and normalize\n",
    "    img_g = img[:,:,1].squeeze().astype(float)/255.0\n",
    "    \n",
    "    # zero padding to apply fully-convolutional network\n",
    "    img_g_padded = np.pad(img_g, ((patch_size[0]//2,patch_size[0]//2),\n",
    "                                  (patch_size[1]//2, patch_size[1]//2)), 'constant', constant_values = [0,0])\n",
    "    \n",
    "    # forward pass\n",
    "    probability = evaluation_fn(np.expand_dims(np.expand_dims(img_g_padded.astype(np.float32), axis=0),axis=0))\n",
    "    probability = probability[0,1,:,:]\n",
    "    \n",
    "    # binary version by thresholding\n",
    "    thresholded_image = probability > threshold\n",
    "    print thresholded_image.shape\n",
    "    \n",
    "    # show results\n",
    "    plt.subplot(1,3,1)\n",
    "    plt.imshow(img_g_padded, cmap='gray')\n",
    "    plt.subplot(1,3,2)\n",
    "    plt.imshow(probability, cmap='jet')\n",
    "    plt.subplot(1,3,3)\n",
    "    plt.imshow(thresholded_image, cmap='gray')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Shift and stitch\n",
    "You can immediately notice that an output is not produced for every pixel value.\n",
    "This is due to the downsampling that is performed in the network. \n",
    "The network acts like a filter of size 'patch_size' (the size of the training patches) with a stride of 2^(number of max pooling operations present in the network). So with 2 max pooling operations in the network we obtain a stride of 4 pixels.\n",
    "\n",
    "If we want to produce an output for every pixel, we can use a **shift-and-stitch** technique, i.e. we shift the input image by one pixel at a time and then stitch the results. More information can be found in this paper: P. Sermanet et al. Overfeat: Integrated recognition, localization and detection using convolutional networks. https://arxiv.org/abs/1312.6229\n",
    "\n",
    "In the next function you will have to implement ```shift_and_stitch()```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def shift_and_stitch(im, patch_size, stride):\n",
    "    ''' Return a full resolution segmentation by segmenting shifted versions\n",
    "    of the input 'im' and stitching those segmentations back together. \n",
    "    The stride determines how many times the image should be shifted in the x and y direction'''\n",
    "    \n",
    "    patch_size = patch_size[0] \n",
    "    \n",
    "    ## create the output shape, the output image is made a bit bigger to allow the stitched versions to be added. \n",
    "    ## the extra parts will be cut off at the end\n",
    "    output = np.zeros([im.shape[0]+2*stride, im.shape[0]+2*stride])\n",
    "    \n",
    "    # the input image has to be padded with zeros to allow shifting of the image. \n",
    "    # pad input image (half filter size + stride)\n",
    "    im_padded = np.pad(im, ((patch_size//2, patch_size//2 + stride),\n",
    "                            (patch_size//2, patch_size//2 + stride)), 'constant', constant_values = [0,0])    \n",
    "    im_p_sh = im_padded.shape\n",
    "    \n",
    "    \n",
    "    # Now implement a loop that:\n",
    "    # - obtains a shifted version of the image\n",
    "    # - applies the fully convolutional network\n",
    "    # - and places the network output in the output of this function\n",
    "        \n",
    "    for row in range(0, stride):\n",
    "        for col in range(0, stride):   \n",
    "    \n",
    "            # >>> YOUR CODE STARTS HERE <<<\n",
    "        \n",
    "            # >>> YOUR CODE ENDS HERE <<<  \n",
    "    \n",
    "    return output[0:im.shape[0], 0:im.shape[1]] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Apply shift-and-stitch\n",
    "When you have succesfully implemented the shift and stitch method you can use it in the following function to segment the retinal vessels at full resolution and submit your results to the challenger framework"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# define these parameters\n",
    "stride = None\n",
    "threshold = None\n",
    "\n",
    "f = 0\n",
    "for img, msk in zip(tes_imgs,tes_msks):  \n",
    "    \n",
    "    img = np.asarray(Image.open(img))\n",
    "    img_g = img[:,:,1].squeeze().astype(float)/255.0  \n",
    "    msk = np.asarray(Image.open(msk))/255\n",
    "    \n",
    "    probability = shift_and_stitch(img_g, patch_size, stride)\n",
    "    output = (probability > threshold) * msk\n",
    "        \n",
    "    plt.subplot(1,3,1)\n",
    "    plt.imshow(img_g)\n",
    "    plt.subplot(1,3,2)\n",
    "    plt.imshow(probability)\n",
    "    plt.subplot(1,3,3)\n",
    "    plt.imshow(output)\n",
    "    plt.show()\n",
    "    \n",
    "    result = Image.fromarray((255*output).astype('uint8'))\n",
    "    result.save(os.path.join(result_output_folder, str(f+1) + \"_mask.png\")) \n",
    "    f += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Submit your results to Challengr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import challenger\n",
    "\n",
    "challenger.submit_results({'username': '',\n",
    "                           'password': ''},\n",
    "                          result_output_folder,\n",
    "                          {'notes': ''})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Additional data augmentation\n",
    "As an optional task, you can implement additional data augmentation by adding functions to the ```SampleExtractor``` class. You can retrain the proposed architecture and check if further data augmentation can improve results.\n",
    "Furthermore, you can modify the network architecture by adding/removing layers, adding dropout, L2 regularization, batch normalization, etc. **Be creative!**\n",
    "\n",
    "\n",
    "# Submission\n",
    "Document all your experiments, add new cells to this notebook if necessary.\n",
    "Submit the notebook fully executed to Freerk (freerk.venhuizen@radboudumc.nl) and Mehmet (mehmet.dalmis@radboudumc.nl). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
