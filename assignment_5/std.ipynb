{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Assignment 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Teaching Assistants:\n",
    "- Gabriel Humpire: g.humpiremamani@radboudumc.nl\n",
    "- Peter Bandi: peter.bandi@radboudumc.nl\n",
    "\n",
    "Please send your notebook **only** to Gabriel and Peter!\n",
    "Submit a notebook **WITH ALL CELLS EXECUTED!!!**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Introduction "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "<img src=\"./figures/lungrads.png\" alt=\"LungRADS guidelines\" align=\"right\" width=\"500\">\n",
    "\n",
    "A pulmonary nodule is a small round or oval-shaped growth in the lung. It may also be called a “spot on the lung” or a “coin lesion.” Pulmonary *nodules* are smaller than three centimeters in diameter. If the growth is larger than that, it is called a pulmonary *mass* and is more likely to represent a cancer than a nodule [http://my.clevelandclinic.org/health/articles/pulmonary-nodules].\n",
    "\n",
    "Nodules can be detected in chest CT images as objects with some kind of rounded shape (even though it is not always the case), which have an intensity that is higher (brighter) than the parenchyma tissue in the lungs.\n",
    "\n",
    "If a nodule is detected, guidelines have to be followed to decide what is the best management for the patient.\n",
    "For this purpose, the LungRADS guidelines have been released [https://www.acr.org/quality-safety/resources/lungrads], which describe the type of follow-up analysis based on the type and size of detected nodules.\n",
    "The main categories of nodules considered in LungRADS are 5:\n",
    "* solid nodule\n",
    "* ground-glass nodules (also called GGN, non-solid nodules)\n",
    "* semi-solid nodules (also called part-solid nodules)\n",
    "* calcified nodules\n",
    "* spiculated nodules\n",
    "\n",
    "<img src=\"./figures/nodules.png\" alt=\"Nodules\" align=\"right\" width=\"500\">\n",
    "\n",
    "**Solid** nodules are characterized by an homogeneous texture, a well-defined shape and an intensity above -450\n",
    "Housfield Units (HU) on CT. **Spiculated** nodules appear as solid lesions with characteristics spikes at the border, often considered as an indicator of malignancy. **Non-Solid** nodules (also called ground-glass opacities) have an intensity on CT lower than solid nodules (above -750 HU). **Part-Solid** nodules (also called semi-solid nodules) contain both a non-solid and a solid part, the latter normally referred to as the solid core. Compared with solid nodules, non-solid and part-solid nodules have a higher frequency of being malignant lesions. Finally, **calcified** nodules are characterized by a high intensity and a well-defined rounded shape on CT. If a nodule is completely calcified, it is a benign lesion.\n",
    "\n",
    "The figure on the right shows the table used in LungRADS, which you can also find in ./literature/AssessmentCategories.pdf.\n",
    "As you can see, the five categories are mentioned in the table, as well as nodule size.\n",
    "While nodule size is something that can be easily measured using a segmentation software, the discrimination of nodule types is not trivial. The figure on the right shows examples of pulmonary nodules at different scales. For each nodule, a 2D view in the axial, coronal and sagittal view is shown. \n",
    "\n",
    "In this assignment, we are going to develop a system based on machine learning to automatically classify pulmonary nodules detected in chest CT scans. For this purpose, we will use data from the publicly available dataset LIDC-IDRI (https://wiki.cancerimagingarchive.net/display/Public/LIDC-IDRI). In LIDC-IDRI, nodules have been annotated and labeled by four radiologists. Based on their annotations, we extracted a subset of nodules that will be used in this assignment for training and for test purposes.\n",
    "\n",
    "The idea of this assignment is to develop a multi-class classification system using machine learning, in particular using neural networks. The goal is to achieve the best classification accuracy on the test set, which contains 50 nodules for each class. For each nodule in both the training and test set, we provide both raw data (cubes of 40x40x40 *mm* containing nodules) and a representation of nodules, meaning a feature vector of 256 values (more details are provided later).\n",
    "The purpose of this assignment is two-fold.\n",
    "* use the features provided to develop a system based on neural networks to classify pulmonary nodule type\n",
    "* design and extract new features from raw data, and use them in your classification framework\n",
    "\n",
    "As an additional (optional) task, you can:\n",
    "* combine your features with the ones provided and investigate possible improvements"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Data "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "We will use data from the publicly available IDC-IDRI dataset (https://wiki.cancerimagingarchive.net/display/Public/LIDC-IDRI).\n",
    "From this dataset, we selected nodules to use in this assignment. In particular, we made a training set and a test set.\n",
    "In both sets, five nodule types are included, namely:\n",
    "* Solid\n",
    "* Non-solid (ground-glass)\n",
    "* Part-solid (semi-solid)\n",
    "* Calcified\n",
    "* Solid spiculated\n",
    "\n",
    "**Training set**. The training set contains a distribution of nodules types that resembles what is typically found in radiology: most of the nodules are solid. As a consequence, you will notice that the distributions of classes are skewed.\n",
    "\n",
    "**Validation set**.\n",
    "We don't provide a validation set. As part of the assignment, you will be asked to build a validation set yourself.\n",
    "\n",
    "**Test set**.\n",
    "The test set contains a balanced distribution of nodules, meaning that we randomly selected the same amount of nodules per class.\n",
    "\n",
    "You can download the data from this link: https://surfdrive.surf.nl/files/index.php/s/Ajopfe8EF0Vb3qg [password: ismi2017]\n",
    "\n",
    "### Data details\n",
    "We provide training and test data in two formats: (a) a **dataset** format and (b) a **raw data** format.\n",
    "\n",
    "#### Dataset format\n",
    "The first format is a typical **dataset** format, a matrix, where each row is a sample and each column is a feature. In this\n",
    "dataset, we provide 256 features per nodule. At this stage, you don't need to care too much about the source of these features (we will tell you more about that later), just consider that these features are somehow descriptive of pulmonary nodules.\n",
    "The files in dataset format are training_set.npz and test_set.npz.\n",
    "In the first part of this assignment, you will be asked to train and validate neural networks using the features provided.\n",
    "The training dataset contains data in the field ['x'] and labels in the field ['y'], while the test set only contains data in the field ['x']. The test set also contains a field ['nodule_ids'], which has unique nodule identifiers that will be used to build the file to submit to *challenger*.\n",
    "The nodule ID has the following format:\n",
    "\n",
    "* seriesuid_worldx_worldy_worldz_diameter\n",
    "\n",
    "where seriesuid is the code that identifies the scan, worldx, y and z indicate the position of the nodule in the scan in world coordinates, and diameter indicates the diameter of the nodule in *mm*. The position is not really important in this assignment, since we extracted the nodules from the scans for you, but in case you want to trace back these nodules, you know where you can find them in the LIDC-IDRI scans. The diameter may be useful in the second part of this assignment.\n",
    "\n",
    "#### Raw data format\n",
    "We also provide raw nodule data, because in the second part of this assignment, you will be asked to train and validate neural networks using raw nodule data. In this part of the assignment, raw data will be available, meaning that you will have the chance of processing data as you like, directly feeding data to the neural network, or extracting additional features, etc.\n",
    "Since pulmonary nodules are extracted from CT scans, raw data is 3D data.\n",
    "Therefore, what we provide is a *cube* for each nodules with size 64x64x64 px, which corresponds to 40x40x40*mm* (we resampled the scans before nodule extraction).\n",
    "\n",
    "In the training set, raw nodule data is organized in folders grouped per nodule type. We think that this may be convenient for you. Cubes are stored as npz files, and HU intensities are stored in the field ['data'], while the class label is stored in the field ['label'] for each nodule. Each file is named using the nodule_id, where we also append the actual nodule label at the end of the file name.\n",
    "\n",
    "In the test set, you will find a similar structure as for the training set, but labels are not provided and nodules are not organized in folders."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Tasks \n",
    "\n",
    "### 1. Train a neural network with given nodule-related features (mandatory)\n",
    "We have computed features of nodules that you can use for this first part of the assignment. The feature vector contains 256 features per sample, you can find them in training/training_set.npz and test/test_set.npz.\n",
    "* report results using neural networks, experiment with different architectures, hyper-parameters, try with different learning rates and report your experience with it\n",
    "\n",
    "### 2. Train a neural network with raw nodule data (mandatory)\n",
    "For the second part of the assignment you will have to deal with raw data, you can find them in the directory 'nodules' for training and test respectively.\n",
    "* report results using neural networks, experiment with several architectures, hyper-parameters\n",
    "* design and extract your own features from the raw data provided\n",
    "\n",
    "### 3. Combine neural networks / features / classifiers to improve the performance (optional)\n",
    "Combine the features we provided for task 1 with the features that you extracted from raw data.\n",
    "Use the combination to train a new neural network, or use a combination of classifiers, if you are familiar with other classification models (you can use the sklearn documentation for that). There is no particular constraint in this last (optional) part. Again, the goal is to do a good job on the test set. Be creative!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Task 1: Train a neural network with given nodule-related features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# import libraries\n",
    "import numpy as np\n",
    "import os\n",
    "import sklearn\n",
    "import sklearn.neighbors\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython import display\n",
    "%matplotlib inline\n",
    "import itertools\n",
    "import time\n",
    "import random\n",
    "\n",
    "# you can get warnings fter importing theano, that is fine.\n",
    "# if you get errors, ask the TAs!\n",
    "import lasagne\n",
    "import theano\n",
    "from theano import tensor as T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "data_dir = 'yourFolder'# define here the directory where you have your data, downloaded from SURFDrive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# labels for the five classes in this classification problem.\n",
    "# These are the same names used in folders for raw training data\n",
    "noduleTypes = [\"Solid\", \"Calcified\", \"PartSolid\", \"GroundGlassOpacity\", \"SolidSpiculated\"]\n",
    "n_classes = len(noduleTypes)\n",
    "print('Classification problem with {} classes'.format(n_classes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# convenience function\n",
    "def get_file_list(path,ext='',queue=''):\n",
    "    if ext != '': return [os.path.join(path,f) for f in os.listdir(path) if f.endswith(''+queue+'.'+ext+'')],\\\n",
    "                         [f for f in os.listdir(path) if f.endswith(''+queue+'.'+ext+'')]    \n",
    "    else: return [os.path.join(path,f) for f in os.listdir(path)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Get to know your data!\n",
    "The data you have now is stored in cubes per nodule, but originally belongs <img src=\"figures/orthogonalviews.jpg\" align='right'> to a Chest CT scan. We have three orthogonal orientations: axial, coronal and sagittal orientations. Each cube is $64\\times64\\times64$ px (40x40x40 *mm*)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Implement a function that, given a cube containing nodule raw 3D data, returns three orthogonal 2D patches, corresponding to the *axial*, *coronal* and *sagittal* view. Use the figure on the right in this notebook as reference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# convenience function that returns the axial, coronal and sagitaal view given a cube (in numpy format) containing a nodule\n",
    "def get_ortogonal_patches(x):\n",
    "    dims = x.shape\n",
    "    axial = None\n",
    "    coronal = None\n",
    "    sagittal= None\n",
    "    return axial, coronal, sagittal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Now visualize some nodules in the training set in axial, coronal and sagittal orientations using the function that you have implemented."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "src_dir = os.path.join(data_dir, \"training\", \"nodules\")\n",
    "for noduleType in noduleTypes:\n",
    "    nodules_dir = os.path.join(src_dir, noduleType)\n",
    "    npzs = get_file_list(nodules_dir, 'npz')\n",
    "    for idx1, f in enumerate(range(len(npzs[0]))):\n",
    "        if idx1 > 5:\n",
    "            continue\n",
    "        file_path = npzs[0][f]\n",
    "        filename = npzs[1][f]\n",
    "        # axes are oriented as (z, x, y)\n",
    "        npz = np.load(file_path)\n",
    "        axial, coronal, sagittal = get_ortogonal_patches(npz['data'])\n",
    "        plt.suptitle(noduleType)\n",
    "        plt.subplot(1,3,1)\n",
    "        plt.imshow(axial   , cmap='gray', vmin=-600-800, vmax=-600+800); plt.title('axial')\n",
    "        plt.subplot(1,3,2)\n",
    "        plt.imshow(coronal , cmap='gray', vmin=-600-800, vmax=-600+800); plt.title('coronal')\n",
    "        plt.subplot(1,3,3)\n",
    "        plt.imshow(sagittal, cmap='gray', vmin=-600-800, vmax=-600+800); plt.title('sagittal')\n",
    "        plt.show()\n",
    "        \n",
    "shape_cube = npz['data'].shape\n",
    "print 'Size of our cubes',shape_cube"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "#### Question\n",
    "Do you think that there is a difference in the quality of patches for the three views? Why?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "*Your answer here.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "#### Question\n",
    "Describe in a few words how each class looks like"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "*Your answer here.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "#### Question\n",
    "Explore the data and identify if there are classes which look similar to other classes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "*Your answer here.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Load datasets\n",
    "Here we are going to load the pre-extracted feature vector of 256 features per sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# load training data (given features)\n",
    "npz = np.load(os.path.join(data_dir, 'training', 'training_set.npz'))\n",
    "x_train = npz['x']\n",
    "y_train = npz['y']\n",
    "print x_train.shape\n",
    "print y_train.shape              "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# load test data (given features)\n",
    "npz = np.load(os.path.join(data_dir, 'test', 'test_set.npz') )\n",
    "x_test = npz['x']\n",
    "nodule_ids_test = npz['nodule_ids']\n",
    "print x_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "As we learned few weeks ago, it is always good to pre-process our data to have zero mean and unit standard deviation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# normalize training data\n",
    "x_mean = np.mean(x_train, axis=0)\n",
    "x_std  = np.std(x_train, axis=0)\n",
    "x_train = (x_train - x_mean)/x_std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# normalize test data\n",
    "x_test = (x_test - x_mean)/x_std"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Training and validation sets\n",
    "\n",
    "The dataset we are given does not explicitly provide a validation set.\n",
    "Therefore, we will have to define one, and consequently update the training set (remove samples used for validation!).\n",
    "\n",
    "In the following cell, you will implement a function that derives a validation set and a new training set from a given (original) training set. You will use this function to define the subsets that you will be using during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# function to split training set into training and validation subsets\n",
    "def split_training_validation_datasets(x, y, val_percentage=0.3, val_balanced=True):\n",
    "    \"\"\"\n",
    "    Derive a training and a validation datasets from a given dataset with\n",
    "    data (x) and labels (y). By default, the validation set is 30% of the\n",
    "    training set, and it has balanced samples across classes. When balancing,\n",
    "    it takes the 30% of the class with less samples as reference.\n",
    "    \"\"\"\n",
    "    x_train = None\n",
    "    y_train = None\n",
    "    x_validation = None\n",
    "    y_validation = None\n",
    "    return x_train, y_train, x_validation, y_validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# split dataset\n",
    "x_train, y_train, x_validation, y_validation = split_training_validation_datasets(x_train, y_train, n_classes)\n",
    "print y_validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "**Shuffle your training set!** This is necessary when we want to train a neural netowkr using stochastic (mini-batch) gradient descent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# shuffle training dataset\n",
    "x_train = None\n",
    "y_train = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "#### Question\n",
    "Why do you think is important to shuffle the data?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Your answer here.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "#### Question\n",
    "How many samples has each class?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "*Your answer here.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### kNN classifier\n",
    "\n",
    "Now that we have defined a training and a validation set, we can define a baseline result by applying kNN classifier (which we have seen in previous assignments), and compute the accuracy on the validation set. If you have made a balanced validation set, accuracy is a good evaluation metric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# kNN\n",
    "classifier = sklearn.neighbors.KNeighborsClassifier(n_neighbors=15)\n",
    "classifier.fit(x_train, y_train)\n",
    "y_validation_auto = classifier.predict(x_validation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Compute the confusion matrix for the results with kNN classifier. In order to compute the **confusion matrix** and the **accuracy**, you can use functions from the sklearn library:\n",
    "\n",
    "* sklearn.metrics.confusion_matrix()\n",
    "* sklearn.metrics.accuracy_score()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# confusion matrix\n",
    "conf_mat_knn  = None\n",
    "\n",
    "# accuracy\n",
    "acc_knn = None\n",
    "\n",
    "print 'Accuracy using kNN: {:.2f}%'.format(100.0*acc_knn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "You can use the following convenience function to visualize the confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(conf_mat, classes, title='Confusion matrix', cmap=plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix\n",
    "    \"\"\"\n",
    "    plt.imshow(conf_mat, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=90)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    thresh = conf_mat.max() / 2.\n",
    "    for i, j in itertools.product(range(conf_mat.shape[0]), range(conf_mat.shape[1])):\n",
    "        plt.text(j, i, conf_mat[i, j], horizontalalignment=\"center\",\n",
    "                 color=\"white\" if conf_mat[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# visualize the confusion matrix\n",
    "plot_confusion_matrix(conf_mat_knn, classes=noduleTypes,\n",
    "                      title='Confusion matrix: kNN as classifier (True label vs. Predicted label)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "source": [
    "#### Question\n",
    "By analyzing the confusion matrix, what are the biggest problems of kNN on this data? Which nodule types are more often confused? Is there a systematic problem? How do you think you can improve this?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "*Your answer here.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Classification with Neural Networks\n",
    "Now that some kind of baseline result has been obtained with kNN, we can start developing a classifier based on neural networks.\n",
    "For this purpose, we will use the Theano library and the Lasagne library, which implements classes and functions that make building and training neural networks easy.\n",
    "Theano is a bit of a special Python library, because it is based on symbolic representation of variables, which you may not be familiar with.\n",
    "For this reason, before we delve into the implementation of our nerual network, we propose a short introduction to Theano and Lasagne, which will clarify some of the doubts and questions you may have about these libraries."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Theano / Lasagne (short) introduction\n",
    "Theano is a Python library that lets you to define, optimize, and evaluate mathematical expressions, especially ones with multi-dimensional arrays (numpy.ndarray).\n",
    "\n",
    "Theano is not a programming language in the normal sense because you write a program in Python that builds expressions for Theano. Still it is like a programming language in the sense that you have to\n",
    "*\tdeclare variables and give their types\n",
    "*\tbuild expressions for how to put those variables together\n",
    "*\tcompile expression graphs to functions in order to use them for computation.\n",
    "\n",
    "It is good to think of theano.function as the interface to a compiler which builds a callable object from a purely symbolic graph. One of Theano’s most important features is that theano.function can optimize a graph and even compile some or all of it into native machine instructions.\n",
    "\n",
    "Theano is a Python library and optimizing compiler for manipulating and evaluating expressions, especially matrix-valued ones. \n",
    "\n",
    "Manipulation of matrices is typically done using the numpy package, so what does Theano do that Python and numpy do not?\n",
    "*\texecution speed optimizations: Theano can use g++ or nvcc to compile parts your expression graph into CPU or GPU instructions, which run much faster than pure Python.\n",
    "*\tsymbolic differentiation: Theano can automatically build symbolic graphs for computing gradients.\n",
    "*\tstability optimizations: Theano can recognize [some] numerically unstable expressions and compute them with more stable algorithms.\n",
    "\n",
    "More about Theano: http://deeplearning.net/software/theano/, https://github.com/Theano/Theano.\n",
    "\n",
    "Lasagne is a lightweight library to build and train neural networks in Theano.\n",
    "\n",
    "More about Lasagne: https://lasagne.readthedocs.io/en/latest/, https://github.com/Lasagne/Lasagne.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Building a Neural Network to classify given features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Now we are going to build a neural network with one hidden layer in lasagne. For this particular assingment we are going to use the fully connected layers, also called Dense Layers in Lasagne (visit the following link to get more details of default parameters http://lasagne.readthedocs.io/en/latest/modules/layers/dense.html).\n",
    "\n",
    "As we have seen in the lecture this week, in order to build our classification framework with neural networks, we have to define and specify parameters for three main components:\n",
    "\n",
    "1. NETWORK ARCHITECTURE\n",
    "2. LOSS FUNCTION\n",
    "3. OPTIMIZATION ALGORITHM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Before we start building our model, we have to define two symbolic variables, for data/features (input) and for labels (targets), which will be used in the definition of the neural network and of the learning algorithm.\n",
    "You can check all tensor types supported by Theano at this link: http://deeplearning.net/software/theano/library/tensor/basic.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# input tensors for data and targets\n",
    "input_var  = T.fmatrix('input')\n",
    "target_var = T.dmatrix('targets')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Creating the neural network\n",
    "Now we can build the architecture of our network.\n",
    "We will build this first network to have **one hidden layer of 10 neurons**.\n",
    "Later, we will experiment with a different number of neurons, hidden layers, etc., but let's start with this one.\n",
    "\n",
    "Keep in mind that the size of the input and of the output layer of your network are given by the data and the classification problem you have to solve.\n",
    "Therefore, before you start building the network, it is good to check again the dimensionality of data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "data_size = x_train.shape\n",
    "n_classes = len(noduleTypes)\n",
    "print data_size\n",
    "print n_classes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Define the architecture of a neural network with one hidden layer of 10 neurons.\n",
    "In your implementation, consider what follows:\n",
    "\n",
    "* it is handy to define a single variable 'network', which initially contains the input layer, and then is passed to other layers as input variable, and also returned as output variable. In this way, the same variable 'network' will be both the input and the output of the current layer. This makes the implementation of a chain of layers easy.\n",
    "\n",
    "* use sigmoid linearity for the hidden layer (later in this assignment you will also be allowed to use ReLU :-) !)\n",
    "\n",
    "* use softmax function as output of the network\n",
    "\n",
    "* use a proper strategy to initialize the parameters of the network (the default initialization of biases in Lasagne is 0, which is fine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# define neural network with 1 hidden layer\n",
    "def build_neural_network(data_size, n_classes):\n",
    "    network = None\n",
    "    \n",
    "    # >>> your code here <<<\n",
    "    \n",
    "    return network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Loss function\n",
    "Now that the architecture is defined, we move to the second component of the learning framework, the **loss function**. In order to do that, we first have to define a function that, given the network, gets the predicted probability for a given input sample.\n",
    "Lasagne offers a function for that, 'get_output()'. Since we are dealing with a multi-class classification problem, categorical cross-entropy seems a reasonable choice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# get the network\n",
    "network = build_neural_network(data_size, n_classes)\n",
    "\n",
    "# get the prediction during training\n",
    "prediction = None\n",
    "\n",
    "# define the (data) loss\n",
    "loss = None\n",
    "loss = loss.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "If we are using categorical cross-entropy as loss function, we need a representation of labels in the training (and later validation) data in a 'one-hot' form. This means that if we have 5 classes, the format of labels has to be the following:\n",
    "\n",
    "* y_train = 1 -> [1, 0, 0, 0, 0]\n",
    "* y_train = 2 -> [0, 1, 0, 0, 0]\n",
    "* y_train = 3 -> [0, 0, 1, 0, 0]\n",
    "* y_train = 4 -> [0, 0, 0, 1, 0]\n",
    "* y_train = 5 -> [0, 0, 0, 0, 1]\n",
    "\n",
    "We have to define a function that convert the given format into a 'one-hot' format. First, check the format of labels in your dataset, then think how you can convert it into a one-hot format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def label_to_one_hot(y, n_classes):\n",
    "    '''\n",
    "    Convert labels into \"one-hot\" representation\n",
    "    '''\n",
    "    y_one_hot = None\n",
    "    \n",
    "    # >>> Your code here <<<\n",
    "    \n",
    "    return y_one_hot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Now we can apply the function that converts labels:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# training\n",
    "print 'Number of samples in training set',y_train.shape\n",
    "y_train_one_hot = label_to_one_hot(y_train, n_classes)\n",
    "print y_train_one_hot.shape\n",
    "\n",
    "# validation\n",
    "print 'Number of samples in validation set',y_validation.shape\n",
    "y_validation_one_hot = label_to_one_hot(y_validation, n_classes)\n",
    "print y_validation_one_hot.shape\n",
    "\n",
    "# check number of samples per class\n",
    "print np.sum(y_train_one_hot, axis=0)\n",
    "print np.sum(y_validation_one_hot, axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Parameters update rule\n",
    "Now that we have defined the architecture and the loss function, we have to specify how we want to train our model. This means that we first have to indicate which parameters of the network we want to optimize (in our case, all weights and all biases), i.e., made them \"trainable\", and then define the algorithm used to update the parameters.\n",
    "In our case, we will use \"Stochastic Gradient Descent\", which is implemented in Lasagne (as an \"update\" rule).\n",
    "\n",
    "As we have seen in the lecture this week, gradient descent algorithms need a 'learning rate', which indicates how much we step in the (opposite) direction of the gradient.\n",
    "We have also seen that strategy to adapt the learning rate during training are possible, but for the moment we just define a fixed learning rate. Pick a value and see what happens, you can optimize this later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# learning rate\n",
    "lr = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# extract the parameters we want to optimize\n",
    "params = lasagne.layers.get_all_params(None) # all parameters made \"trainable\"\n",
    "\n",
    "# indicate that we will train using SGD\n",
    "updates = lasagne.updates.sgd(None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Check validation performance during training\n",
    "All the main components required to train our network have been defined now.\n",
    "However, we have seen that in order to properly monitor the behaviour of a network during training, we should check the performance (the loss) on a separate validation set.\n",
    "For this purpose, we have to define a function in Theano to measure the loss and the accuracy on the validation set during training. This is similar to what done for the training loss.\n",
    "\n",
    "Please note that evaluating on the validation set is actually a testing procedure.\n",
    "For some reasons, in this case a flag 'deterministic=True' has to be set.\n",
    "The exact reason will become clear next week..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# get the prediction on the validation set during training\n",
    "val_prediction = lasagne.layers.get_output(network, input_var, deterministic=True)\n",
    "val_loss = lasagne.objectives.categorical_crossentropy(val_prediction, target_var)\n",
    "val_loss = val_loss.mean()\n",
    "\n",
    "# compute the (mean) accuracy\n",
    "val_acc  = T.mean(T.eq(T.argmax(val_prediction, axis=1), T.argmax(target_var, axis=1)), dtype=theano.config.floatX)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Now we will define the theano functions that make use of all the rules that we have defined so far. This means that Theano will compile the function, which may take a while (if you are using a GPU)...\n",
    "These functions are based on a symbolic representation of the variables we have defined so far. Since this may be the first time you are working with this kind of variables, we provide the definition of these functions, which are based on theano.function().\n",
    "In order to better understand what these functions are doing, check the documentation of theano.function() on the Theano website at this link: http://deeplearning.net/software/theano/library/compile/function.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_fn  = theano.function([input_var, target_var], loss, updates=updates, name='train')\n",
    "val_fn    = theano.function([input_var, target_var], [val_loss, val_acc]  , name='validation')\n",
    "get_preds = theano.function([input_var]            , val_prediction       , name='get_preds')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Sanity check\n",
    "As a sanity check for what you have implemented so far, you can run the network to classify the validation set, and measure the accuracy. Think if what you get makes sense.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "val_loss, val_acc = val_fn(x_validation.astype(np.float32), y_validation_one_hot.astype(np.float32))\n",
    "print ('Initial validation accuracy = {:.2f}%'.format(100.*val_acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Learning procedure\n",
    "Now we can write the learning algorithm, as we have seen in the lecture this week.\n",
    "Basically, we will iteratively update the parameters of our network by extracting mini-batches from the training set, until all the training samples have been used. After a complete round, one epoch is done. We repeat this procedure for a number of epochs that you define.\n",
    "During the training loop, we also want to check the performance of the trained network on the validation set.\n",
    "Therefore, for each epoch, after a training pass, we also classify the validation set.\n",
    "\n",
    "We provide the main structure of the learning script, implement the missing parts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# learning algorithm\n",
    "n_epochs   = None\n",
    "batch_size = None # adapt this value based on the memory of your GPU\n",
    "n_mini_batch_training = None # number of training mini-batches given the batch_size\n",
    "\n",
    "# lists where we will be storing values during training, for visualization purposes\n",
    "tra_losses = []\n",
    "val_losses = []\n",
    "val_accs   = []\n",
    "\n",
    "# we want to save the parameters that give the best performance on the validation set\n",
    "# therefore, we store the best validation accuracy, and save the parameters to disk\n",
    "best_val_acc = 0\n",
    "\n",
    "# loop over the number of epochs\n",
    "for epoch in xrange(n_epochs):\n",
    "    \n",
    "    st = time.time()\n",
    "    \n",
    "    # training\n",
    "    cum_tra_loss = 0.0 # cumulative training loss\n",
    "    for b in range(n_mini_batch_training):\n",
    "        x_batch = None # extract a mini-batch from x_train\n",
    "        y_batch = None # extract labels for the mini-batch\n",
    "        mini_batch_loss = train_fn(x_batch, y_batch)\n",
    "        cum_tra_loss += mini_batch_loss\n",
    "        \n",
    "    # validation\n",
    "    val_loss, val_acc = val_fn(x_validation.astype(np.float32), y_validation_one_hot.astype(np.float32))\n",
    "    # if the accuracy improves, save the network parameters\n",
    "    if val_acc > best_val_acc:\n",
    "        best_val_acc = val_acc\n",
    "        # save network\n",
    "        params = lasagne.layers.get_all_param_values(network)\n",
    "        np.savez('./nn_params.npz', params=params)\n",
    "    \n",
    "    tra_loss = None # final training loss for this epoch\n",
    "    \n",
    "    # add to lists\n",
    "    tra_losses.append(tra_loss)\n",
    "    val_losses.append(val_loss)\n",
    "    val_accs.append(val_acc)\n",
    "    en = time.time()\n",
    "\n",
    "    # plot learning curves\n",
    "    fig = plt.figure(figsize=(10, 5))\n",
    "    tra_loss_plt, = plt.plot(range(len(tra_losses)), tra_losses, 'b')\n",
    "    val_loss_plt, = plt.plot(range(len(val_losses)), val_losses, 'g')\n",
    "    val_acc_plt, = plt.plot(range(len(val_accs)), val_accs, 'r')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.ylabel('loss')\n",
    "    plt.legend([tra_loss_plt, val_loss_plt, val_acc_plt], \n",
    "               ['training loss', 'validation loss', 'validation accuracy'],\n",
    "               loc='center left', bbox_to_anchor=(1, 0.5))\n",
    "    plt.title('Best validation accuracy = {:.2f}%'.format(100. * best_val_acc))\n",
    "    display.clear_output(wait=True)\n",
    "    display.display(plt.gcf())\n",
    "    time.sleep(.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Classification: validation set\n",
    "Now we can use the trained network to classify the validation set, and check that the performance corresponds to the best value obtained during training. We can compute the accuracy and also visualize the confusion matrix, to get a feeling how well we are doing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# load the network architecture again\n",
    "network = build_neural_network(data_size, n_classes) # define the network again, in case you start here\n",
    "npz = np.load('./nn_params.npz') # load stored parameters\n",
    "lasagne.layers.set_all_param_values(network, npz['params']) # set parameters\n",
    "\n",
    "# compile the function again, using the (re)loaded network\n",
    "val_prediction = lasagne.layers.get_output(network, input_var, deterministic=True)\n",
    "get_preds = theano.function([input_var]            , val_prediction       , name='get_preds')\n",
    "\n",
    "# classify validation set\n",
    "prediction = get_preds(x_validation.astype(np.float32))\n",
    "y_validation_auto = np.argmax(prediction, axis=1)\n",
    "conf_mat_nn  = sklearn.metrics.confusion_matrix(np.argmax(y_validation_one_hot, axis=1), y_validation_auto)\n",
    "acc_nn = sklearn.metrics.accuracy_score(y_validation, y_validation_auto)\n",
    "print('Accuracy on validation set: {:.2f}%'.format(100. * acc_nn))\n",
    "plot_confusion_matrix(conf_mat_nn, classes=noduleTypes,\n",
    "                      title='Confusion matrix: Neural Network classifier (True label vs. Predicted label)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Use the labels in the validation set to identify the cases that you are misclassifying, and see what the network says about those cases. Use the functions provided at the beginning of this notebook to visualize nodules that have been misclassified. Since the labels in our dataset are given by humans (no ground truth available, only reference standard), there can be some confusion in the way nodules are classified, even in the reference standard.\n",
    "\n",
    "#### Question\n",
    "Based on what you have learned about the appearnce of nodules at the beginning of this notebook, do you think you agree with the labels predicted by your network? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "*Your answer here.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Classification: test set\n",
    "Now we can repeat the classification step on the test set, and submit the results to challenger. During the test procedure, we will save the predictions in a csv file, which will be submitted to challenger.\n",
    "Please note that the reference standard in challenger has labels y = [1, ..., 5]. Take this into account when making the csv file for your submission."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# load the network architecture again\n",
    "network = build_neural_network(data_size, n_classes)\n",
    "npz = np.load('./nn_params.npz')\n",
    "lasagne.layers.set_all_param_values(network, npz['params'])\n",
    "\n",
    "# classify test set\n",
    "n_test_samples = x_test.shape[0]\n",
    "h_csv = open('./nn_results.csv', 'w')\n",
    "h_csv.write('nodule_id, label\\n')\n",
    "for n in range(n_test_samples):\n",
    "    test_sample = None\n",
    "    nodule_id   = nodule_ids_test[n]\n",
    "    prediction  = None\n",
    "    y = np.argmax(prediction[0])\n",
    "    h_csv.write('{}, {}\\n'.format(nodule_id, None))\n",
    "h_csv.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Submission to challenger\n",
    "Now we can submit the classification output file to challenger, put a short comment describing your submission."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import challenger\n",
    "\n",
    "challenger.submit_results({'username': '',\n",
    "                           'password': ''},\n",
    "                          \"nn_results.csv\",\n",
    "                          {'notes': 'my first submission'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Hyper-parameter optimization\n",
    "So far, we have implemented a simple network with one hidden layer, 10 neurons and sigmoid activation function.\n",
    "We also used a constant learning rate, and we have not used regularization.\n",
    "This means that there is plenty of room for improvement!\n",
    "For example:\n",
    "* change the architecture of your network, add neurons, add layers\n",
    "* change the activation function, try ReLU and see what happens\n",
    "* change the learning rate, or try to find a strategy to adapt it during training\n",
    "* try some kind of regularization (L1, L2)\n",
    "\n",
    "In order to fine-tune these parameters, you may want to expand the main script used for training and validation to include a search for the optimal set of hyper-parameters (cross-validation).\n",
    "For each experiment:\n",
    "* provide a clear description of the setup (value, range of parameters used in the search), which we can read and understand when we will be grading your assignment\n",
    "* save the trained network, which you can load later\n",
    "\n",
    "In particular, show and explain:\n",
    "* how the results change by changing the learning parameters/architecture of the network\n",
    "* how the learning rate worked and how it affected the performance of the neural network\n",
    "\n",
    "Note that hyper-parameters tuning has to be done using the **validation** set.\n",
    "When you are happy ith the performance on the vaidation set, you can classify the test set and submit the results to challenger!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Task 2: Train a neural network with raw nodule data\n",
    "Now that you have developed your supervised learning framework using the features that we provided, repeat the procedure using raw data as input. You can use the functions provided at the beginning of this notebook to extract 2D views from 3D nodules, which could be useful to develop your network.\n",
    "\n",
    "Repeat the training procedure, tune the hyper-parameters, and submit the new results.\n",
    "\n",
    "**MAKE NEW CELLS AND WRITE NEW CODE FOR THIS SECOND PART!!!**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Task 3: Improve the perfomance (optional)\n",
    "In this optional task, you can try to improve the performance of your system by combining different types of features, different neural networks etc. Be creative!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
